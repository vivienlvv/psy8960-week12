---
title: "PSY8960 Week 12 NLP"
author: "Vivien Lee"
date: "2023-04-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Script Settings and Resources
```{r}
# Importing relevant libraries 
library(tidyverse)
library(httr)
library(rvest)
library(tictoc)
```

# Data Import and Cleaning

### Data Creation through webscraping
```{r}
# # Web Scraping from old IO Psychology subreddit 
# 
# ## Creating tibble to store response from get request, setting initial URL and counter
# responses_tbl = tibble() 
# get_url = "https://old.reddit.com/r/IOPsychology/"
# counter = 1 
# 
# ## This is a while-loop goes through all visible pages in the io subreddit 
# ### I initially set it to 100 pages but ran into an error at page 40 because 
# ### there are only 40 pages right now, so I set 40 for the while loop 
# 
# while(counter <= 40){
#   response = content(GET(get_url, user_agent("UMN Student lee02903@umn.edu")), as = "text")
#   responses_tbl = bind_rows(responses_tbl, tibble(response)) # storing get response to tibble
#   # print(paste(counter, "Just received data from", get_url))
#   
#   next_url = read_html(response) %>% html_elements("span.next-button a") %>% html_attr("href")
#   get_url = next_url # Setting URL for next iteration
#   counter = counter + 1 # Incrementing counter for while loop
#   
#   Sys.sleep(2) # Pause to conform to rate limit
# }
```

```{r}
# # Creating a function "get_info" that gets three things:
# # 1) post title, 2) num of upvotes for each page, and 3) time
# get_info = function(single_page){
#   single_page = read_html(single_page)
#   xpath_post = "//div[contains(@class, 'odd') or contains(@class, 'even')]//a[contains(@class, 'title may-blank')]"
#   xpath_upvotes = "//div[contains(@class, 'odd') or contains(@class, 'even')]//div[@class = 'score unvoted']"
#   
#   post = html_elements(single_page, xpath = xpath_post) %>% html_text()
#   upvotes = html_elements(single_page, xpath = xpath_upvotes) %>% 
#     html_text() %>% 
#     as.numeric() 
#   
#   # Added for myself  to see the post date
#   time =  html_elements(single_page, "time:nth-child(1)") %>% html_text() 
#   
#   io_tbl = tibble(time = time[-1], post, upvotes)
#   return(io_tbl)
# }
```

```{r}
# # Outputting a list of posts containing a separate tibble for each page
# posts_ls = lapply(responses_tbl$response, get_info)
# 
# # Collapsing all tibbles in the list to form one giant tibble
# week12_tbl =  bind_rows(posts_ls) %>% select(-time)
# 
# # Saving scraped dataset as week12_tbl.csv
# write_csv(week12_tbl, "../data/week12_tbl.csv")
```

```{r import, message= FALSE}
# Importing saved data 
week12_tbl = read_csv("../data/week12_tbl.csv")
```
\n

### Creating text corpus using NLP
```{r message = FALSE}
library(tm)
library(tidytext)
library(qdap)
```

Create a new lemmatized pre-processed corpus from io_corpus_original called io_corpus by:

1. removing references to IO Psychology (this is the topic of the reddit, so this isn’t useful – remember to also remove variations on this term and double-check your success in the data),
2. completing any other beneficial pre-processing steps as appropriate to the dataset.
(Be sure to explain in your comments why you chose the steps you chose and the order you chose for them.)
```{r creating_corpus}
# Creating a volatile corpus 
## Getting dataframe with right colnames in the right order
df_source = week12_tbl %>% 
  mutate(doc_id = 1:nrow(week12_tbl)) %>%
  rename(text = post) %>% 
  select(doc_id, text)
io_source = DataframeSource(df_source)
io_corpus_original = VCorpus(io_source)
```

```{r}
# Cleaning corpus 

## Adding to contractions 
extra = data.frame(contraction = str_replace_all(contractions$contraction, "'", "’"),
                   expanded = contractions$expanded)
contractions = rbind(contractions, extra)


io_corpus = io_corpus_original %>%
  # This first step is added because reddit apostrophes look weird and don't 
  # match the one in the contractinos dictionary
  tm_map(content_transformer(function(x) str_replace_all(x, pattern = "\\’", replacement = "\\'"))) %>%
  tm_map(content_transformer(replace_abbreviation)) %>% 
  tm_map(content_transformer(replace_contraction)) %>% 
  tm_map(tolower) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeWords, c(stopwords("en"), "io psychology", "io", "io psych", "io psyc")) %>%
  tm_map(stripWhitespace) %>%
  tm_map(stemDocument, language = "english")
```

```{r}
# Checking processing
io_corpus_original$content[[565]]$content
io_corpus$content[[565]]
```


# Visualization
# Analysis
# Publication