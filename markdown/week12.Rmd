---
title: "PSY8960 Week 12 NLP"
author: "Vivien Lee"
date: "2023-04-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Script Settings and Resources
```{r message = FALSE}
# Importing relevant libraries 
library(tidyverse)
library(httr)
library(rvest)

library(tm)
library(tidytext)
library(qdap)

library(RWeka)
library(doParallel)

library(topicmodels)
library(wordcloud)
```

# Data Import and Cleaning

### Data Creation through webscraping
```{r}
# # Web Scraping from old IO Psychology subreddit 
# 
# ## Creating tibble to store response from get request, setting initial URL and counter
# responses_tbl = tibble() 
# get_url = "https://old.reddit.com/r/IOPsychology/"
# counter = 1 
# 
# ## This is a while-loop goes through all visible pages in the io subreddit 
# ### I initially set it to 100 pages but ran into an error at page 40 because 
# ### there are only 40 pages right now, so I set 40 for the while loop 
# 
# while(counter <= 40){
#   response = content(GET(get_url, user_agent("UMN Student lee02903@umn.edu")), as = "text")
#   responses_tbl = bind_rows(responses_tbl, tibble(response)) # storing get response to tibble
#   # print(paste(counter, "Just received data from", get_url))
#   
#   next_url = read_html(response) %>% html_elements("span.next-button a") %>% html_attr("href")
#   get_url = next_url # Setting URL for next iteration
#   counter = counter + 1 # Incrementing counter for while loop
#   
#   Sys.sleep(2) # Pause to conform to rate limit
# }
```

```{r}
# # Creating a function "get_info" that gets three things:
# # 1) post title, 2) num of upvotes for each page, and 3) time
# get_info = function(single_page){
#   single_page = read_html(single_page)
#   xpath_post = "//div[contains(@class, 'odd') or contains(@class, 'even')]//a[contains(@class, 'title may-blank')]"
#   xpath_upvotes = "//div[contains(@class, 'odd') or contains(@class, 'even')]//div[@class = 'score unvoted']"
#   
#   post = html_elements(single_page, xpath = xpath_post) %>% html_text()
#   upvotes = html_elements(single_page, xpath = xpath_upvotes) %>% 
#     html_text() %>% 
#     as.numeric() 
#   
#   # Added for myself  to see the post date
#   time =  html_elements(single_page, "time:nth-child(1)") %>% html_text() 
#   
#   io_tbl = tibble(time = time[-1], post, upvotes)
#   return(io_tbl)
# }
```

```{r}
# # Outputting a list of posts containing a separate tibble for each page
# posts_ls = lapply(responses_tbl$response, get_info)
# 
# # Collapsing all tibbles in the list to form one giant tibble
# week12_tbl =  bind_rows(posts_ls) %>% select(-time)
# 
# # Saving scraped dataset as week12_tbl.csv
# write_csv(week12_tbl, "../data/week12_tbl.csv")
```

```{r import, message= FALSE}
# Importing saved data 
week12_tbl = read_csv("../data/week12_tbl.csv")
```
\n

### Creating text corpus using NLP

Create a new lemmatized pre-processed corpus from io_corpus_original called io_corpus by:

1. removing references to IO Psychology (this is the topic of the reddit, so this isn’t useful – remember to also remove variations on this term and double-check your success in the data),
2. completing any other beneficial pre-processing steps as appropriate to the dataset.
(Be sure to explain in your comments why you chose the steps you chose and the order you chose for them.)
```{r creating_corpus}
# Creating a volatile corpus 
io_corpus_original = VCorpus(VectorSource(week12_tbl$post))
```

```{r cleaning_corpus}
# Cleaning corpus 

## Creating custom stopword list 
io_stopwords = c("io psychology", "io", "io psych", "io psyc", "riopsychology")
custom_stopwords = c(stopwords("en"), io_stopwords)

## Begin actual cleaning
io_corpus = io_corpus_original %>%
  # This step is added because reddit apostrophes look weird and don't 
  # match the one in the contractions dictionary
  tm_map(content_transformer(str_replace_all), pattern = "’", replacement = "'") %>%
  tm_map(content_transformer(str_replace_all), pattern = "—|“|”|‘", replacement = "") %>% 
  tm_map(content_transformer(replace_abbreviation)) %>% 
  tm_map(content_transformer(replace_contraction)) %>% 
  tm_map(content_transformer(str_to_lower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, custom_stopwords) %>%
  tm_map(stripWhitespace) %>% 
  tm_map(content_transformer(trimws)) # removing leading and ending spaces
```

```{r}
# Checking processing
io_corpus_original[[106]]$content
io_corpus[[106]]$content

# Visualizing cleaned corpus in df
# data.frame(text=unlist(sapply(io_corpus, `[`, "content")), stringsAsFactors=F) 
```

### Creating function "compare_them" to compare individual post titles in the corpus before and after cleaning
```{r compare_corpus}
# Writing compare_them() function 
## input: io_corpus_original and io_corpus
compare_them = function(corpus1, corpus2){
  select_index = sample(1:length(io_corpus$content), 1)
  original_row = corpus1[[select_index]]$content
  cleaned_row = corpus2[[select_index]]$content
  print(original_row)
  print(cleaned_row)
}
compare_them(io_corpus_original, io_corpus)
```

## NOTE!!! Need to change sparsity value after adding additional processing steps 



## Create Bigram document-term matrix 

* I first created a function called "bigram_filter" to filter out empty posts and posts with less than 2 words because they won't have any bigram and their document-term matrix will just be zeroes. 
```{r filter_posts}
# Creating function to filter out empty posts and posts with less than 2 words (because won't have a term for bigram)
## This functinon KEEPS the posts we want
## I created this because when trying to use the FindTopicsNumber function, 
## it threw an error saying some of my documents have 0 frequency across all bigrams which led me to build this function. I did not build this into my pre-processing pipeline because this would mess up the order of post for cleaned vs. original corpora.

bigram_filter = function(x){
  bool = nchar(stripWhitespace(x$content)[[1]]) > 0 & str_count((x$content)[[1]], "\\S+") >= 2
  return(bool)
}

# This returns a logical vector showing whether they were filtered out which could 
# be used for matching upvotes later
post_retained = sapply(io_corpus, bigram_filter)
```

* To create bigram document-term matrix, I first by built a cutsom tokenizer to get only bigrams. To reduce sparsity in the slim document-term matrix, I used the "removeSparseTerm()" function and set multiple sparsity values until I got one that was close to an n:k ratio that's close to 2:1 and 3:1 by dividing number of rows by the number of columns in the document-term matrix. 
```{r create_dtm}
# Creating Bigram DTM 

## Creating bigram tokenizer using Weka
bigram_Tokenizer <- function(x) { 
  NGramTokenizer(x, Weka_control(min=2, max=2))
  }

## Creating bigram DTM 
io_dtm = io_corpus %>% 
  tm_filter(bigram_filter) %>% # removing empty posts and 
  DocumentTermMatrix(control=list(tokenize = bigram_Tokenizer)) 

# need to change after additional processing 
io_slim_dtm = removeSparseTerms(io_dtm, sparse = 0.998) 
```


# Analysis

## Building topic models 

## NEED TO COME BACK TO THIS- HAVEN'T ANSWERED To answer the following questions:
Q1. Using the beta matrix alone, what topics would you conclude your final topic list maps onto? (e.g., topic 1, 2, 3…n each reflect what substantive topic construct? Use your best judgment.)

Q2. Look at the original text of documents with the highest and lowest probabilities assigned to each document. Do your topic names derived from your interpretation of the beta matrix conceptually match with the content of the original posts? What kind of validity evidence does your answer to this question represent?

#### Finding optimal number of topics 
* Add reason why this number of topics (k) was picked
```{r lda_number}
# Finding optimal number of lda topics; adopted code from slide 25
library(ldatuning)

## Turning on parallelization
local_cluster = makeCluster(detectCores() - 1)   
registerDoParallel(local_cluster)

## Finding # of topics
tuning = FindTopicsNumber(io_dtm,
                          topics = seq(2,15,1),
                           metrics = c("Griffiths2004",
                                       "CaoJuan2009",
                                       "Arun2010",
                                       "Deveaud2014"),
                          verbose = TRUE)
FindTopicsNumber_plot(tuning)

## Turning off parallelization
stopCluster(local_cluster)
registerDoSEQ()
```

#### Fitting topic model with k = 
```{r build_lda}
# Actual fitting of LDA model 
lda_results = LDA(io_dtm, 5)

## Posterior probabilities representing the probability that a word belongs to a topic
lda_betas = tidy(lda_results, matrix = "beta")

## Posterior probabilities of documents about each topic
lda_gammas = tidy(lda_results, matrix = "gamma")
```

#### Creating required tibble "topics_tbl" 
```{r}
# Creating required tibble "topics_tbl"

## First getting only the posts used in DTM using the logical vector I defined
## above to track which posts were filtered out
post_titles = week12_tbl %>% filter(post_retained) %>% 
  pull(post)

topics_tbl = lda_gammas %>% group_by(document) %>% 
  top_n(1, gamma) %>%
  slice(1) %>%
  ungroup() %>% 
  mutate(document = as.numeric(document)) %>% 
  arrange(document) %>%
  mutate(original = post_titles) %>% 
  rename(doc_id = document, 
         probability = gamma) %>% 
  select(doc_id, original, topic, probability)
```

## Step 11: create dataset called final_tbl
```{r}
upvotes_filtered = week12_tbl[topics_tbl$doc_id, "upvotes"]$upvotes

final_tbl = topics_tbl %>% 
  mutate(upvotes = upvotes_filtered,
         topic = factor(topic, ordered = FALSE))
```

## Step 12: Running statistical analysis to determine if upvotes differs by topic (double check)
```{r}
# Anova (this)
anova_mod = aov(upvotes ~ topic, data = final_tbl)
summary(anova_mod)

# # Chi square
# chisq_tbl = final_tbl %>% group_by(topic) %>%
#   summarize(avg_upvotes = mean(upvotes, na.rm = TRUE))# there was NA because one of the posts was very new and did not have any upvote
```


\n
\n
\n
\n

# Visualization

## Creating a wordcloud of io_dtm
* Based on the wordcloud below, we can see that
```{r}
m = as.matrix(io_dtm) 
wordcloud(words = colnames((m)),
          freq = colSums(m),
          min.freq = 1, max.words = 75,
          colors = c("#f29d35", "#f24b99"))
```

\n
\n
\n
\n

# Resources 
* This section contains links or forums I consulted to solve rJava problem related to library "qdap" on a Macbook M1 machine
```{r}
# reference this: https://stackoverflow.com/questions/67849830/how-to-install-rjava-package-in-mac-with-m1-architecture
#https://stackoverflow.com/questions/46513639/how-to-downgrade-java-from-9-to-8-on-a-macos-eclipse-is-not-running-with-java-9
```

