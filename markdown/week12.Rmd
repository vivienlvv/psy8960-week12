---
title: "PSY8960 Week 12 NLP"
author: "Vivien Lee"
date: "2023-04-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Script Settings and Resources
```{r}
# Importing relevant libraries 
library(tidyverse)
library(httr)
library(rvest)
library(RWeka)
```

# Data Import and Cleaning

### Data Creation through webscraping
```{r}
# # Web Scraping from old IO Psychology subreddit 
# 
# ## Creating tibble to store response from get request, setting initial URL and counter
# responses_tbl = tibble() 
# get_url = "https://old.reddit.com/r/IOPsychology/"
# counter = 1 
# 
# ## This is a while-loop goes through all visible pages in the io subreddit 
# ### I initially set it to 100 pages but ran into an error at page 40 because 
# ### there are only 40 pages right now, so I set 40 for the while loop 
# 
# while(counter <= 40){
#   response = content(GET(get_url, user_agent("UMN Student lee02903@umn.edu")), as = "text")
#   responses_tbl = bind_rows(responses_tbl, tibble(response)) # storing get response to tibble
#   # print(paste(counter, "Just received data from", get_url))
#   
#   next_url = read_html(response) %>% html_elements("span.next-button a") %>% html_attr("href")
#   get_url = next_url # Setting URL for next iteration
#   counter = counter + 1 # Incrementing counter for while loop
#   
#   Sys.sleep(2) # Pause to conform to rate limit
# }
```

```{r}
# # Creating a function "get_info" that gets three things:
# # 1) post title, 2) num of upvotes for each page, and 3) time
# get_info = function(single_page){
#   single_page = read_html(single_page)
#   xpath_post = "//div[contains(@class, 'odd') or contains(@class, 'even')]//a[contains(@class, 'title may-blank')]"
#   xpath_upvotes = "//div[contains(@class, 'odd') or contains(@class, 'even')]//div[@class = 'score unvoted']"
#   
#   post = html_elements(single_page, xpath = xpath_post) %>% html_text()
#   upvotes = html_elements(single_page, xpath = xpath_upvotes) %>% 
#     html_text() %>% 
#     as.numeric() 
#   
#   # Added for myself  to see the post date
#   time =  html_elements(single_page, "time:nth-child(1)") %>% html_text() 
#   
#   io_tbl = tibble(time = time[-1], post, upvotes)
#   return(io_tbl)
# }
```

```{r}
# # Outputting a list of posts containing a separate tibble for each page
# posts_ls = lapply(responses_tbl$response, get_info)
# 
# # Collapsing all tibbles in the list to form one giant tibble
# week12_tbl =  bind_rows(posts_ls) %>% select(-time)
# 
# # Saving scraped dataset as week12_tbl.csv
# write_csv(week12_tbl, "../data/week12_tbl.csv")
```

```{r import, message= FALSE}
# Importing saved data 
week12_tbl = read_csv("../data/week12_tbl.csv")
```
\n

### Creating text corpus using NLP
```{r message = FALSE}
library(tm)
library(tidytext)
library(qdap)
library(topicmodels)
```

Create a new lemmatized pre-processed corpus from io_corpus_original called io_corpus by:

1. removing references to IO Psychology (this is the topic of the reddit, so this isn’t useful – remember to also remove variations on this term and double-check your success in the data),
2. completing any other beneficial pre-processing steps as appropriate to the dataset.
(Be sure to explain in your comments why you chose the steps you chose and the order you chose for them.)
```{r creating_corpus}
# Creating a volatile corpus 
io_corpus_original = VCorpus(VectorSource(week12_tbl$post))
```

```{r}
# Cleaning corpus 

## Creating custom stopword list 
io_stopwords = c("io psychology", "io", "io psych", "io psyc", "riopsychology")
custom_stopwords = c(stopwords("en"), io_stopwords)

## Begin actual cleaning
io_corpus = io_corpus_original %>%
  # This step is added because reddit apostrophes look weird and don't 
  # match the one in the contractions dictionary
  tm_map(content_transformer(str_replace_all), pattern = "’", replacement = "'") %>%
  tm_map(content_transformer(str_replace_all), pattern = "—|“|”|‘", replacement = "") %>% 
  tm_map(content_transformer(replace_abbreviation)) %>% 
  tm_map(content_transformer(replace_contraction)) %>% 
  tm_map(content_transformer(str_to_lower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, custom_stopwords) %>%
  tm_map(stripWhitespace) %>% 
  tm_map(content_transformer(trimws)) # removing leading and ending spaces
```

```{r}
# Checking processing
io_corpus_original[[106]]$content
io_corpus[[106]]$content

# Visualizing cleaned corpus in df
# data.frame(text=unlist(sapply(io_corpus, `[`, "content")), stringsAsFactors=F) 
```

7. Write a function called compare_them() that takes two corpora as input and displays one randomly selected row of content from each (e.g., if 251 was randomly selected, display the content of row 251 from both corpora). Then use this function to display the same row from both corpora you just created.
Hint 1. 
- You will probably want to run this function 10-20 times as a check to be sure your preprocessed corpus looks like you want it to look.
```{r}
# Writing compare_them() function 
## input: io_corpus_original and io_corpus
compare_them = function(corpus1, corpus2){
  select_index = sample(1:length(io_corpus$content), 1)
  original_row = corpus1[[select_index]]$content
  cleaned_row = corpus2[[select_index]]$content
  print(original_row)
  print(cleaned_row)
}
compare_them(io_corpus_original, io_corpus)
```
## NEED TO REFINE CORPUS CLEANING PIPELINE!!!


8. Create a bigram DTM called io_dtm. Also create a version of this DTM with sparse terms eliminated called io_slim_dtm. Retain between a 2:1 and 3:1 N/k ratio in the slim DTM.
- Hint 2. Aggressive or incorrect pre-processing can create a lot of mystery errors that are difficult to interpret. One common issue is that you may end up with empty entries in your corpus, e.g., if pre-processed cases end up containing zero text. If this happens, you can remove those cases using this code:
tm_filter(corpus, FUN = function(x) { return(nchar(stripWhitespace(x$content)[[1]]) > 0) })
However, be warned that this will make Step 11 below a bit more complicated than it would be otherwise. It may be easier/better to simply pre-process less aggressively.
- Hint 3. Another common issue is that you do/do not use content_transformer() when you should not/should have done so. Be sure you understand what this function does.

## Need to change sparsity value after adding additional processing steps 

```{r}
# Creating function to filter out empty posts and posts with less than 2 words (because won't have a term for bigram)
## This functinon KEEPS the posts we want
## I created this because when trying to use the FindTopicsNumber function, 
## it threw an error saying some of my documents have 0 frequency across all bigrams which led me to build this function. I did not build this into my pre-processing pipeline because this would mess up the order of post for cleaned vs. original corpora.

bigram_filter = function(x){
  bool = nchar(stripWhitespace(x$content)[[1]]) > 0 & str_count((x$content)[[1]], "\\S+") >= 2
  return(bool)
}

# This returns a logical vector showing whether they were filtered out which could 
# be used for matching upvotes later
post_retained = sapply(io_corpus, bigram_filter)
```


```{r}
# Creating Bigram DTM 

## Creating bigram tokenizer using Weka
bigram_Tokenizer <- function(x) { 
  NGramTokenizer(x, Weka_control(min=2, max=2))
  }

## Creating bigram DTM 
io_dtm = io_corpus %>% 
  tm_filter(bigram_filter) %>% # removing empty posts and 
  DocumentTermMatrix(control=list(tokenize = bigram_Tokenizer)) 

# need to change after additional processing 
io_slim_dtm = removeSparseTerms(io_dtm, sparse = 0.998) 
```


# Analysis


9. Using good practices in topic modeling (and remember to explain everything clearly in comments), use latent Dirichlet allocation to categorize posts into topics from io_dtm. Create a tibble topics_tbl.
- Include the following columns:
1. doc_id, a document number identifier, which should match the one contained in io_corpus (i.e., it should reflect the original line number from your original input vector/data frame)
2. original, the original post title
3. topic, the topic number assigned by the LDA
4. probability, the probability that document belongs to that topic
## Building topic models 
```{r}
# Finding optimal number of lda topics; adopted code from slide 25
library(ldatuning)

## Turning on parallelization
local_cluster = makeCluster(detectCores() - 1)   
registerDoParallel(local_cluster)

## Finding # of topics
tuning = FindTopicsNumber(io_dtm,
                          topics = seq(2,15,1),
                           metrics = c("Griffiths2004",
                                       "CaoJuan2009",
                                       "Arun2010",
                                       "Deveaud2014"),
                          verbose = TRUE)
FindTopicsNumber_plot(tuning)

## Turning off parallelization
stopCluster(local_cluster)
registerDoSEQ()
```

```{r}
# Actual fitting of LDA model 
lda_results = LDA(io_dtm, 5)

## Posterior probabilities representing the probability that a word belongs to a topic
lda_betas = tidy(lda_results, matrix = "beta")

## Posterior probabilities of documents about each topic
lda_gammas = tidy(lda_results, matrix = "gamma")

```

```{r}
# Creating required tibble 

## First getting only the posts used in DTM using the logical vector I defined
## above to track which posts were filtered out
post_titles = week12_tbl %>% filter(post_retained) %>% 
  pull(post)

topics_tbl = lda_gammas %>% group_by(document) %>% 
  top_n(1, gamma) %>%
  slice(1) %>%
  ungroup() %>% 
  mutate(document = as.numeric(document)) %>% 
  arrange(document) %>%
  mutate(original = post_titles) %>% 
  rename(doc_id = document, 
         probability = gamma) %>% 
  select(doc_id, original, topic, probability)
```

# Visualization

# Publication




# Resources 
* This section contains links or forums I consulted to solve rJava problem related to library "qdap" on a Macbook M1 machine
```{r}
# reference this: https://stackoverflow.com/questions/67849830/how-to-install-rjava-package-in-mac-with-m1-architecture
#https://stackoverflow.com/questions/46513639/how-to-downgrade-java-from-9-to-8-on-a-macos-eclipse-is-not-running-with-java-9
```

